{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "NLP Sentiment Analysis Model Documentation\n",
    "\n",
    "Overview:\n",
    "-----------\n",
    "This document provides documentation for the NLP Sentiment Analysis model trained using linear regression. The model predicts sentiment labels for text data, indicating whether a given text expresses positive, negative, or neutral sentiment.\n",
    "\n",
    "Model Details:\n",
    "--------------\n",
    "- Model Type:Logistic Regression\n",
    "- Feature Extraction: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- Data Cleaning: Removed irrelevant characters, HTML tags, and non-alphabetic characters.\n",
    "- Preprocessing: Downsampled the dataset, applied stemming, and tokenization.\n",
    "- Evaluation Metric: Accuracy,Precison,F1-Score\n",
    "\n",
    "Usage:\n",
    "------\n",
    "1. Import the required libraries and modules.\n",
    "2. Load the pre-trained model and necessary preprocessing components.\n",
    "3. Input a text string to get the sentiment prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Neccessary Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import chardet\n",
    "\n",
    "rawdata = open('train.csv', 'rb').read()\n",
    "\n",
    "result = chardet.detect(rawdata)\n",
    "encoding = result['encoding']\n",
    "\n",
    "train_data = pd.read_csv('train.csv', encoding=encoding)\n",
    "rawdata = open('test.csv', 'rb').read()\n",
    "result = chardet.detect(rawdata)\n",
    "encoding = result['encoding']\n",
    "test_data =  pd.read_csv('test.csv', encoding=encoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
      "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
      "7  50e14c0bb8                                         Soooo high   \n",
      "8  e050245fbd                                        Both of you   \n",
      "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
      "\n",
      "                                       selected_text sentiment Time of Tweet  \\\n",
      "0                I`d have responded, if I were going   neutral       morning   \n",
      "1                                           Sooo SAD  negative          noon   \n",
      "2                                        bullying me  negative         night   \n",
      "3                                     leave me alone  negative       morning   \n",
      "4                                      Sons of ****,  negative          noon   \n",
      "5  http://www.dothebouncy.com/smf - some shameles...   neutral         night   \n",
      "6                                                fun  positive       morning   \n",
      "7                                         Soooo high   neutral          noon   \n",
      "8                                        Both of you   neutral         night   \n",
      "9                       Wow... u just became cooler.  positive       morning   \n",
      "\n",
      "  Age of User              Country  Population -2020  Land Area (Km²)  \\\n",
      "0        0-20          Afghanistan          38928346         652860.0   \n",
      "1       21-30              Albania           2877797          27400.0   \n",
      "2       31-45              Algeria          43851044        2381740.0   \n",
      "3       46-60              Andorra             77265            470.0   \n",
      "4       60-70               Angola          32866272        1246700.0   \n",
      "5      70-100  Antigua and Barbuda             97929            440.0   \n",
      "6        0-20            Argentina          45195774        2736690.0   \n",
      "7       21-30              Armenia           2963243          28470.0   \n",
      "8       31-45            Australia          25499884        7682300.0   \n",
      "9       46-60              Austria           9006398          82400.0   \n",
      "\n",
      "   Density (P/Km²)  \n",
      "0               60  \n",
      "1              105  \n",
      "2               18  \n",
      "3              164  \n",
      "4               26  \n",
      "5              223  \n",
      "6               17  \n",
      "7              104  \n",
      "8                3  \n",
      "9              109  \n"
     ]
    }
   ],
   "source": [
    "print(train_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting The stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text sentiment\n",
      "0                I`d have responded, if I were going   neutral\n",
      "1      Sooo SAD I will miss you here in San Diego!!!  negative\n",
      "2                          my boss is bullying me...  negative\n",
      "3                     what interview! leave me alone  negative\n",
      "4   Sons of ****, why couldn`t they put them on t...  negative\n",
      "                                                text sentiment\n",
      "0  Last session of the day  http://twitpic.com/67ezh   neutral\n",
      "1   Shanghai is also really exciting (precisely -...  positive\n",
      "2  Recession hit Veronique Branquinho, she has to...  negative\n",
      "3                                        happy bday!  positive\n",
      "4             http://twitpic.com/4w75p - I like it!!  positive\n"
     ]
    }
   ],
   "source": [
    "#extracting the releveant fields\n",
    "train_data = train_data[['text','sentiment']]\n",
    "test_data = test_data[['text','sentiment']]\n",
    "print(train_data.head(5))\n",
    "print(test_data.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         1\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         1281\n",
       "sentiment    1281\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     11117\n",
       "positive     8582\n",
       "negative     7781\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for ditributions\n",
    "train_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, target_column,random_seed=42):\n",
    "    # Separate the dataset into three classes\n",
    "    class_1 = df[df[target_column] == \"neutral\"]\n",
    "    class_2 = df[df[target_column] == \"positive\"]\n",
    "    class_3 = df[df[target_column] == \"negative\"]\n",
    "\n",
    "    # Find the minimum number of samples among the three classes\n",
    "    min_samples = min(len(class_1), len(class_2), len(class_3))\n",
    "\n",
    "  \n",
    "    class_1_downsampled = class_1.sample(n=min_samples, random_state=random_seed)\n",
    "    class_2_downsampled = class_2.sample(n=min_samples, random_state=random_seed)\n",
    "\n",
    "    # Combine the downsampled classes with the original class 3\n",
    "    downsampled_dataset = pd.concat([class_1_downsampled, class_2_downsampled, class_3], axis=0)\n",
    "\n",
    "    return downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     7781\n",
       "positive    7781\n",
       "negative    7781\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = downsample(train_data,\"sentiment\")\n",
    "train_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #Reaplacing Classes with Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7781\n",
       "1    7781\n",
       "2    7781\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.replace({'sentiment':{'neutral' : 0}},inplace=True)\n",
    "train_data.replace({'sentiment':{'positive' : 1}},inplace=True)\n",
    "train_data.replace({'sentiment':{'negative' : 2}},inplace=True)\n",
    "\n",
    "train_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_data.replace({'sentiment':{'neutral' : 0}},inplace=True)\n",
    "test_data.replace({'sentiment':{'positive' : 1}},inplace=True)\n",
    "test_data.replace({'sentiment':{'negative' : 2}},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "stem = PorterStemmer()\n",
    "def stem_data(txt):\n",
    "    stemed_data = re.sub('[^a-zA-Z]',' ',txt)\n",
    "    stemed_data =stemed_data.lower()\n",
    "    stemed_data = stemed_data.split()\n",
    "    stemed_data = [stem.stem(wrd) for wrd in stemed_data if not wrd in stopwords.words('english')]\n",
    "    stemed_data = ' '.join(stemed_data)\n",
    "    return stemed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"stemmed_data\"] = train_data['text'].apply(stem_data)\n",
    "test_data[\"stemmed_data\"] = test_data['text'].apply(stem_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stemmed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>i have to go to the doctor... i don`t want to....</td>\n",
       "      <td>0</td>\n",
       "      <td>go doctor want caus wait sooo long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8316</th>\n",
       "      <td>Can`t sleep but is happy that the Fugees are k...</td>\n",
       "      <td>0</td>\n",
       "      <td>sleep happi fuge keep compani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12622</th>\n",
       "      <td>Job???????   Not so much.</td>\n",
       "      <td>0</td>\n",
       "      <td>job much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>Nope wasn`t kidding at all.  Sometimes I thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>nope kid sometim think forest gump run year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18694</th>\n",
       "      <td>_from_hell  how`s monday for you?</td>\n",
       "      <td>0</td>\n",
       "      <td>hell monday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment  \\\n",
       "4634   i have to go to the doctor... i don`t want to....          0   \n",
       "8316   Can`t sleep but is happy that the Fugees are k...          0   \n",
       "12622                          Job???????   Not so much.          0   \n",
       "21585   Nope wasn`t kidding at all.  Sometimes I thin...          0   \n",
       "18694                  _from_hell  how`s monday for you?          0   \n",
       "\n",
       "                                      stemmed_data  \n",
       "4634            go doctor want caus wait sooo long  \n",
       "8316                 sleep happi fuge keep compani  \n",
       "12622                                     job much  \n",
       "21585  nope kid sometim think forest gump run year  \n",
       "18694                                  hell monday  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stemmed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>0</td>\n",
       "      <td>last session day http twitpic com ezh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>1</td>\n",
       "      <td>shanghai also realli excit precis skyscrap gal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>2</td>\n",
       "      <td>recess hit veroniqu branquinho quit compani shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy bday!</td>\n",
       "      <td>1</td>\n",
       "      <td>happi bday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>1</td>\n",
       "      <td>http twitpic com w p like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  Last session of the day  http://twitpic.com/67ezh          0   \n",
       "1   Shanghai is also really exciting (precisely -...          1   \n",
       "2  Recession hit Veronique Branquinho, she has to...          2   \n",
       "3                                        happy bday!          1   \n",
       "4             http://twitpic.com/4w75p - I like it!!          1   \n",
       "\n",
       "                                        stemmed_data  \n",
       "0              last session day http twitpic com ezh  \n",
       "1  shanghai also realli excit precis skyscrap gal...  \n",
       "2  recess hit veroniqu branquinho quit compani shame  \n",
       "3                                         happi bday  \n",
       "4                          http twitpic com w p like  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['stemmed_data']\n",
    "X_test = test_data['stemmed_data']\n",
    "Y_train = train_data['sentiment']\n",
    "Y_test = test_data['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4634                    go doctor want caus wait sooo long\n",
       "8316                         sleep happi fuge keep compani\n",
       "12622                                             job much\n",
       "21585          nope kid sometim think forest gump run year\n",
       "18694                                          hell monday\n",
       "                               ...                        \n",
       "27464    rec game tri cri pain much need lose heart bre...\n",
       "27470      lol know haha fall asleep get bore shaun p joke\n",
       "27472             http twitpic com vr want visit anim late\n",
       "27476    wish could come see u denver husband lost job ...\n",
       "27477    wonder rake client made clear net forc dev lea...\n",
       "Name: stemmed_data, Length: 23343, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   last session day http twitpic com ezh\n",
       "1       shanghai also realli excit precis skyscrap gal...\n",
       "2       recess hit veroniqu branquinho quit compani shame\n",
       "3                                              happi bday\n",
       "4                               http twitpic com w p like\n",
       "                              ...                        \n",
       "3529                                    im tire sleep tri\n",
       "3530    alon old hous thank net keep aliv kick whoever...\n",
       "3531    know mean littl dog sink depress want move som...\n",
       "3532             sutra next youtub video gonna love video\n",
       "3533          http twitpic com woj omgssh ang cute ng bbi\n",
       "Name: stemmed_data, Length: 3534, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test.apply(lambda x: np.str_(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8914)\t0.36817848539472803\n",
      "  (0, 14019)\t0.43344927711716036\n",
      "  (0, 16506)\t0.3391339620257018\n",
      "  (0, 2448)\t0.41215330696670416\n",
      "  (0, 16548)\t0.2914133609251895\n",
      "  (0, 4132)\t0.5012938309581979\n",
      "  (0, 6042)\t0.2355140813916517\n",
      "  (1, 3062)\t0.47654798689629824\n",
      "  (1, 8151)\t0.3678335109095725\n",
      "  (1, 5653)\t0.672527679392639\n",
      "  (1, 6542)\t0.278479343421971\n",
      "  (1, 13779)\t0.3282625476277222\n",
      "  (2, 10055)\t0.6227872810344516\n",
      "  (2, 7898)\t0.7823912081444391\n",
      "  (3, 17366)\t0.27414336079123575\n",
      "  (3, 12925)\t0.30121303976938374\n",
      "  (3, 6363)\t0.5098384353251276\n",
      "  (3, 5455)\t0.45579911304304593\n",
      "  (3, 15187)\t0.2191606644531618\n",
      "  (3, 13994)\t0.32598728495138685\n",
      "  (3, 8223)\t0.29522323834850395\n",
      "  (3, 10554)\t0.3535140641712389\n",
      "  (4, 9881)\t0.6685743798122914\n",
      "  (4, 6757)\t0.7436452774398622\n",
      "  (5, 16244)\t0.47586399283826103\n",
      "  :\t:\n",
      "  (23340, 3014)\t0.245180619435804\n",
      "  (23340, 7099)\t0.22714398385317702\n",
      "  (23340, 16548)\t0.23939802939328295\n",
      "  (23341, 225)\t0.4191160775105941\n",
      "  (23341, 7171)\t0.40963822396374144\n",
      "  (23341, 3836)\t0.46062591942873715\n",
      "  (23341, 3263)\t0.2712255319928227\n",
      "  (23341, 8977)\t0.31649491091205556\n",
      "  (23341, 16966)\t0.246830528112759\n",
      "  (23341, 13256)\t0.23077670832068217\n",
      "  (23341, 3023)\t0.24944725079233704\n",
      "  (23341, 7898)\t0.30695692517480067\n",
      "  (23342, 2469)\t0.35046707999788923\n",
      "  (23342, 12280)\t0.35046707999788923\n",
      "  (23342, 3895)\t0.313320011097153\n",
      "  (23342, 264)\t0.3367572021245965\n",
      "  (23342, 5447)\t0.2726104166585829\n",
      "  (23342, 8497)\t0.3035926979430023\n",
      "  (23342, 2872)\t0.2743447824220226\n",
      "  (23342, 2884)\t0.2823377298127336\n",
      "  (23342, 10326)\t0.25567784243221636\n",
      "  (23342, 8596)\t0.22616135743518567\n",
      "  (23342, 9176)\t0.1956098172531219\n",
      "  (23342, 17033)\t0.20403320933969232\n",
      "  (23342, 10345)\t0.15777617208803732\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15819)\t0.4216152924489614\n",
      "  (0, 13340)\t0.6141841803949164\n",
      "  (0, 8528)\t0.3817007200566433\n",
      "  (0, 7099)\t0.3230056748580287\n",
      "  (0, 3656)\t0.27098043571911073\n",
      "  (0, 3014)\t0.34865432092694937\n",
      "  (1, 15760)\t0.33308179973035895\n",
      "  (1, 13364)\t0.3605047323733574\n",
      "  (1, 12371)\t0.17563393729028123\n",
      "  (1, 11870)\t0.39402631740697847\n",
      "  (1, 6085)\t0.15282795271473062\n",
      "  (1, 5760)\t0.39402631740697847\n",
      "  (1, 4922)\t0.2333832991536303\n",
      "  (1, 2704)\t0.34262287704498345\n",
      "  (1, 1569)\t0.4100677046629347\n",
      "  (1, 458)\t0.2331565768773938\n",
      "  (2, 13384)\t0.43850667403746013\n",
      "  (2, 12403)\t0.5622442625387076\n",
      "  (2, 12204)\t0.38641173018013353\n",
      "  (2, 6882)\t0.38894424862326826\n",
      "  (2, 3062)\t0.4370373351431049\n",
      "  (3, 6542)\t0.5093419220339135\n",
      "  (3, 1280)\t0.8605642372645976\n",
      "  (4, 15819)\t0.5959532911969049\n",
      "  (4, 8748)\t0.43989811574930365\n",
      "  :\t:\n",
      "  (3530, 6463)\t0.2184963125053067\n",
      "  (3530, 440)\t0.2359478846573929\n",
      "  (3530, 402)\t0.29787899521976496\n",
      "  (3531, 16548)\t0.18864610314425578\n",
      "  (3531, 15641)\t0.4499987307588632\n",
      "  (3531, 13988)\t0.4499987307588632\n",
      "  (3531, 13672)\t0.3956094814487298\n",
      "  (3531, 9990)\t0.2619780586412623\n",
      "  (3531, 9470)\t0.2541578027678333\n",
      "  (3531, 8815)\t0.24107565335897302\n",
      "  (3531, 8325)\t0.18966015448702167\n",
      "  (3531, 4143)\t0.2776267799982582\n",
      "  (3531, 3849)\t0.313869885231544\n",
      "  (3532, 17497)\t0.4023692953512695\n",
      "  (3532, 16334)\t0.7690138968986082\n",
      "  (3532, 10375)\t0.3164497289205618\n",
      "  (3532, 9006)\t0.23253666963706826\n",
      "  (3532, 6082)\t0.304142800137606\n",
      "  (3533, 15819)\t0.2892093201279607\n",
      "  (3533, 10384)\t0.4705471973318921\n",
      "  (3533, 7099)\t0.2215675126026699\n",
      "  (3533, 3496)\t0.33745064674834946\n",
      "  (3533, 3014)\t0.23916134191732455\n",
      "  (3533, 1265)\t0.49800048796922464\n",
      "  (3533, 558)\t0.47620958528480845\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=900)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=900)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=900)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLPmodel = LogisticRegression(max_iter=900)\n",
    "NLPmodel.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = \n",
      "Training  Accuracy : 82.50867497750932\n",
      "Training Precision : 82.65964050126286\n",
      "Training F1 Score : 82.55765571597043\n"
     ]
    }
   ],
   "source": [
    "#training  accuracy\n",
    "train_prediction = NLPmodel.predict(X_train)\n",
    "\n",
    "print(f\"Training  Accuracy : {accuracy_score(Y_train,train_prediction)*100}\")\n",
    "print(f\"Training Precision : {precision_score(Y_train,train_prediction,average='weighted')*100}\")\n",
    "print(f\"Training F1 Score : {f1_score(Y_train,train_prediction,average='weighted')*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing  Accuracy : 71.1092246745897\n",
      "Testing Precision : 71.2650000694683\n",
      "Testing F1 Score : 71.09874204086508\n"
     ]
    }
   ],
   "source": [
    "#testing accuracy\n",
    "\n",
    "\n",
    "test_prediction = NLPmodel.predict(X_test)\n",
    "\n",
    "print(f\"Testing  Accuracy : {accuracy_score(Y_test,test_prediction)*100}\")\n",
    "print(f\"Testing Precision : {precision_score(Y_test,test_prediction,average='weighted')*100}\")\n",
    "print(f\"Testing F1 Score : {f1_score(Y_test,test_prediction,average='weighted')*100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle   \n",
    "model = \"NLPModel.sav\"\n",
    "pickle.dump(NLPmodel,open(model,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment and Testing of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "#deployment of the model\n",
    "classes = [\"neutral\",\"positive\",\"negative\"]\n",
    "txt = \"Today is a bad day\"\n",
    "#load the pretrained model\n",
    "SentimentModel = pickle.load(open(\"NLPModel.sav\",'rb'))\n",
    "txt = stem_data(txt)\n",
    "vectorized_text = vectorizer.transform([txt])\n",
    "sentimentscore= SentimentModel.predict(vectorized_text)\n",
    "if(sentimentscore == 0):\n",
    "    print(\"Neutral\")\n",
    "elif(sentimentscore == 1):\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
